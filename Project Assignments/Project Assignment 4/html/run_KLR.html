
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>run_KLR</title><meta name="generator" content="MATLAB 9.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2016-12-16"><meta name="DC.source" content="run_KLR.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><pre class="codeinput">clear <span class="string">all</span>;
data = load(<span class="string">'heartstatlog_trainSet.txt'</span>);
labels = load(<span class="string">'heartstatlog_trainLabels.txt'</span>);
dataTest = load(<span class="string">'heartstatlog_testSet.txt'</span>);
labelTest = load(<span class="string">'heartstatlog_testLabels.txt'</span>);
data = bsxfun(@rdivide,bsxfun(@minus,data,mean(data)),std(data));

labels = 2*(labels - 1.5);
labelTest = 2*(labelTest - 1.5);

<span class="comment">%Formulating Linear functions</span>
kernel = data*data';
kernelTest = data*dataTest';
C = [0.01 0.1 0.25 1 5 25 100];

<span class="keyword">for</span> j = 1:size(C,2)

k = 5;
Ntrain=length(data);
ind = randperm(Ntrain);

testInd=ind(1:floor(Ntrain/k))';
trainData = data;
trainLabels = labels;

trainData(testInd,:) = [];
trainLabels(testInd,:) = [];

testData = data(testInd,:);
testLabels = labels(testInd,:);

kernelCVTrain = kernel;
kernelCVTrain(testInd,:) = [];
kernelCVTrain(:,testInd) = [];

kernelCVTest = kernel(1:size(trainData,1),testInd);
kernelCVVal = kernel(1:size(testData,1),testInd);

kernelTrain = data(1:size(trainData,1),:)*data(1:size(trainData,1),:)';

kernelTestCV = kernelTest(1:size(trainData,1),:);

alpha = ones(size(kernelCVTrain,1),1)*(0.5)*(1/C(j));
fun3 = @(alpha)objFun3(alpha,trainLabels,kernelCVTrain,C(j));

A = [];
b = [];
Aeq = trainLabels';
beq = 0;
lb = zeros(size(kernelCVTrain,1),1);
ub = C(j)*ones(size(trainLabels,1),1);

opAlpha = fmincon(fun3,alpha,A,b,Aeq,beq,lb,ub);

supporters = find(opAlpha &gt; 1e-5);
b = 0;
bias = @(b)objBias3(b,opAlpha,trainLabels,kernelCVTrain,C(j));

opB(j) = fminunc(bias,b);

predVal = sign((((opAlpha.*trainLabels)/C(j))'*kernelCVTest)' + opB(j));
errorsCVValid(:,j) = sum(predVal ~= testLabels)/length(testLabels);

predTrain = sign((((opAlpha.*trainLabels)/C(j))'*kernelTrain)' + opB(j));
errorsCVTrain(:,j) = sum(predTrain ~= labels(1:length(predTrain)))/length(predTrain);

predTest = sign((((opAlpha.*trainLabels)/C(j))'*kernelTestCV)' + opB(j));
errorsTestCV(:,j) = sum(predTest ~= labelTest)/length(labelTest);

<span class="keyword">end</span>

[minErrorCV,lambdaInd] = min(errorsCVValid);
bestLambda = C(lambdaInd);
bestB = opB(lambdaInd);

<span class="comment">%running on entire training data</span>
A = [];
b = [];
Aeq = labels';
beq = 0;
lb = zeros(size(kernel,1),1);
ub = bestLambda*ones(size(labels,1),1);
alpha = ones(size(kernel,1),1)*(0.5)*(1/bestLambda);
fun3 = @(alpha)objFun3(alpha,labels,kernel,bestLambda);

trainAlpha = fmincon(fun3,alpha,A,b,Aeq,beq,lb,ub);

supporters = find(trainAlpha &gt; 1e-5);
b = 0;
bias = @(b)objBias3(b,trainAlpha,labels,kernel,bestLambda);

trainB = fminunc(bias,b);

pred = sign((((trainAlpha.*labels)/bestLambda)'*kernel)' + trainB);
errorsTrain = sum(pred ~= labels)/length(labels);

<span class="comment">%running on testing data</span>

pred = sign((((trainAlpha.*labels)/bestLambda)'*kernelTest)' + bestB);
errorsTest = sum(pred ~= labelTest)/length(labelTest);

figure
plot(log10(C),errorsCVTrain,<span class="string">'-x'</span>);
hold <span class="string">on</span>
plot(log10(C),errorsCVValid,<span class="string">'-o'</span>);
hold <span class="string">on</span>

plot(log10(C),errorsTestCV,<span class="string">'--gs'</span>,<span class="string">'LineWidth'</span>,2,<span class="string">'MarkerSize'</span>,10,<span class="string">'MarkerEdgeColor'</span>,<span class="string">'b'</span>);
legend(<span class="string">'Train'</span>,<span class="string">'Validation'</span>,<span class="string">'Test'</span>);
xlabel(<span class="string">'Lambda'</span>);
ylabel(<span class="string">'Error'</span>);
</pre><pre class="codeoutput">
Solver stopped prematurely.

fmincon stopped because it exceeded the function evaluation limit,
options.MaxFunctionEvaluations = 3000 (the default value).

Warning: Gradient must be provided for trust-region algorithm; using
quasi-newton algorithm instead. 

Local minimum possible.

fminunc stopped because it cannot decrease the objective function
along the current search direction.




Solver stopped prematurely.

fmincon stopped because it exceeded the function evaluation limit,
options.MaxFunctionEvaluations = 3000 (the default value).

Warning: Gradient must be provided for trust-region algorithm; using
quasi-newton algorithm instead. 

Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the optimality tolerance.




Local minimum possible.

fminunc stopped because it cannot decrease the objective function
along the current search direction.




Solver stopped prematurely.

fmincon stopped because it exceeded the function evaluation limit,
options.MaxFunctionEvaluations = 3000 (the default value).

Warning: Gradient must be provided for trust-region algorithm; using
quasi-newton algorithm instead. 

Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the optimality tolerance.




Solver stopped prematurely.

fmincon stopped because it exceeded the function evaluation limit,
options.MaxFunctionEvaluations = 3000 (the default value).

Warning: Gradient must be provided for trust-region algorithm; using
quasi-newton algorithm instead. 

Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the optimality tolerance.




Solver stopped prematurely.

fmincon stopped because it exceeded the function evaluation limit,
options.MaxFunctionEvaluations = 3000 (the default value).

Warning: Gradient must be provided for trust-region algorithm; using
quasi-newton algorithm instead. 

Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the optimality tolerance.




Solver stopped prematurely.

fmincon stopped because it exceeded the function evaluation limit,
options.MaxFunctionEvaluations = 3000 (the default value).

Warning: Gradient must be provided for trust-region algorithm; using
quasi-newton algorithm instead. 

Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the optimality tolerance.




Solver stopped prematurely.

fmincon stopped because it exceeded the function evaluation limit,
options.MaxFunctionEvaluations = 3000 (the default value).

Warning: Gradient must be provided for trust-region algorithm; using
quasi-newton algorithm instead. 

Local minimum possible.

fminunc stopped because it cannot decrease the objective function
along the current search direction.




Solver stopped prematurely.

fmincon stopped because it exceeded the function evaluation limit,
options.MaxFunctionEvaluations = 3000 (the default value).

Warning: Gradient must be provided for trust-region algorithm; using
quasi-newton algorithm instead. 

Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the optimality tolerance.



</pre><img vspace="5" hspace="5" src="run_KLR_01.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016a</a><br></p></div><!--
##### SOURCE BEGIN #####
clear all;
data = load('heartstatlog_trainSet.txt');
labels = load('heartstatlog_trainLabels.txt');
dataTest = load('heartstatlog_testSet.txt');
labelTest = load('heartstatlog_testLabels.txt');
data = bsxfun(@rdivide,bsxfun(@minus,data,mean(data)),std(data));

labels = 2*(labels - 1.5);
labelTest = 2*(labelTest - 1.5);

%Formulating Linear functions
kernel = data*data';
kernelTest = data*dataTest';        
C = [0.01 0.1 0.25 1 5 25 100];

for j = 1:size(C,2)
        
k = 5;
Ntrain=length(data);
ind = randperm(Ntrain);

testInd=ind(1:floor(Ntrain/k))';
trainData = data;
trainLabels = labels;

trainData(testInd,:) = [];
trainLabels(testInd,:) = [];

testData = data(testInd,:);
testLabels = labels(testInd,:);

kernelCVTrain = kernel;
kernelCVTrain(testInd,:) = [];
kernelCVTrain(:,testInd) = [];

kernelCVTest = kernel(1:size(trainData,1),testInd);
kernelCVVal = kernel(1:size(testData,1),testInd);

kernelTrain = data(1:size(trainData,1),:)*data(1:size(trainData,1),:)';

kernelTestCV = kernelTest(1:size(trainData,1),:);

alpha = ones(size(kernelCVTrain,1),1)*(0.5)*(1/C(j));
fun3 = @(alpha)objFun3(alpha,trainLabels,kernelCVTrain,C(j));

A = [];
b = [];
Aeq = trainLabels';
beq = 0;
lb = zeros(size(kernelCVTrain,1),1);
ub = C(j)*ones(size(trainLabels,1),1);

opAlpha = fmincon(fun3,alpha,A,b,Aeq,beq,lb,ub);

supporters = find(opAlpha > 1e-5);
b = 0;
bias = @(b)objBias3(b,opAlpha,trainLabels,kernelCVTrain,C(j));

opB(j) = fminunc(bias,b);

predVal = sign((((opAlpha.*trainLabels)/C(j))'*kernelCVTest)' + opB(j));
errorsCVValid(:,j) = sum(predVal ~= testLabels)/length(testLabels);

predTrain = sign((((opAlpha.*trainLabels)/C(j))'*kernelTrain)' + opB(j));
errorsCVTrain(:,j) = sum(predTrain ~= labels(1:length(predTrain)))/length(predTrain);

predTest = sign((((opAlpha.*trainLabels)/C(j))'*kernelTestCV)' + opB(j));
errorsTestCV(:,j) = sum(predTest ~= labelTest)/length(labelTest);

end

[minErrorCV,lambdaInd] = min(errorsCVValid);
bestLambda = C(lambdaInd);
bestB = opB(lambdaInd);

%running on entire training data
A = [];
b = [];
Aeq = labels';
beq = 0;
lb = zeros(size(kernel,1),1);
ub = bestLambda*ones(size(labels,1),1);
alpha = ones(size(kernel,1),1)*(0.5)*(1/bestLambda);
fun3 = @(alpha)objFun3(alpha,labels,kernel,bestLambda);

trainAlpha = fmincon(fun3,alpha,A,b,Aeq,beq,lb,ub);

supporters = find(trainAlpha > 1e-5);
b = 0;
bias = @(b)objBias3(b,trainAlpha,labels,kernel,bestLambda);

trainB = fminunc(bias,b);

pred = sign((((trainAlpha.*labels)/bestLambda)'*kernel)' + trainB);
errorsTrain = sum(pred ~= labels)/length(labels);

%running on testing data

pred = sign((((trainAlpha.*labels)/bestLambda)'*kernelTest)' + bestB);
errorsTest = sum(pred ~= labelTest)/length(labelTest);

figure
plot(log10(C),errorsCVTrain,'-x');
hold on
plot(log10(C),errorsCVValid,'-o');
hold on

plot(log10(C),errorsTestCV,'REPLACE_WITH_DASH_DASHgs','LineWidth',2,'MarkerSize',10,'MarkerEdgeColor','b');
legend('Train','Validation','Test');
xlabel('Lambda');
ylabel('Error');
##### SOURCE END #####
--></body></html>